{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_SA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kaOyn7SzbxQW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IMDB Sentiment Analysis Challenge"
      ]
    },
    {
      "metadata": {
        "id": "sgb4slkF-Vsb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook, I will create and train two SA models that use the IMDB dataset.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "First install necessary packages and download and extract The IMBD dataset."
      ]
    },
    {
      "metadata": {
        "id": "IJC4TQPqzW1A",
        "colab_type": "code",
        "outputId": "8583879f-7dab-48ff-f678-907ce6656fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "\n",
        "try:\n",
        "  print(\"GPU device found\")\n",
        "  device = torch.device(\"cuda\")\n",
        "except:\n",
        "  print(\"GPU device not found, I advise changing the runtime type.\")\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "url = \"https://resemble.sfo2.digitaloceanspaces.com/imdb-review-dataset.zip\"\n",
        "path = os.path.join(\"imdb_dataset\")\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "  fpath = os.path.join(path, \"imdb-review-dataset.zip\")\n",
        "  urlretrieve(url, fpath)\n",
        "  review_data = zipfile.ZipFile(fpath, 'r')\n",
        "  review_data.extractall(path)\n",
        "  review_data.close()\n",
        "\n",
        "fpath = os.path.join(path, \"imdb_master.csv\")\n",
        "f = open(fpath)\n",
        "movieReviews = pd.read_csv(f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU device found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cv2fzRTWGbt6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data preprocessing step, here I group data by its respective train/test label and shuffle the training and testing data. Next  I use keras's tokenizer to vectorize each review into a sequence of integers corresponding to the diction from the reviews."
      ]
    },
    {
      "metadata": {
        "id": "Bgs9b0QoHj4g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Take the first half of dataset, because the latter reviews are unlabeled\n",
        "reviews = movieReviews['review'].tolist()[0:49999]\n",
        "sentiments = [0 if sentiment == \"neg\" else 1 for sentiment in movieReviews['label'].tolist()[0:49999]]\n",
        "\n",
        "#assort training and test data accordingly to the dataset \n",
        "types = movieReviews['type'].tolist()[0:49999]\n",
        "training_idxs = []\n",
        "testing_idxs = []\n",
        "for i in range(49999):\n",
        "  if types[i] == \"test\":\n",
        "    testing_idxs.append(i)\n",
        "  if types[i] == \"train\":\n",
        "    training_idxs.append(i)\n",
        "\n",
        "random.shuffle(training_idxs)\n",
        "random.shuffle(testing_idxs)\n",
        "\n",
        "x_train_temp, y_train = [reviews[i] for i in training_idxs], [sentiments[i] for i in training_idxs]\n",
        "x_test_temp, y_test = [reviews[i] for i in testing_idxs], [sentiments[i] for i in testing_idxs]\n",
        "\n",
        "#vectorize each review into a sequence of integers corresponding to words from reviews\n",
        "vocab_size = 15000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "x_train_tokenized = tokenizer.texts_to_sequences(x_train_temp)\n",
        "x_test_tokenized = tokenizer.texts_to_sequences(x_test_temp)\n",
        "\n",
        "#set a maximum review length as well as pad sequences with zeros under the review length\n",
        "#make sure to tokenize the train and test set seperately\n",
        "x_train = pad_sequences(x_train_tokenized, maxlen = 250)\n",
        "x_test = pad_sequences(x_test_tokenized, maxlen = 250)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIrdZ1hIbCUI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here I use Keras to implement simple LSTM model with dropout. I used a binary cross entropy as my loss function (since it's a binary classification problem), and the classic adam optimizer. "
      ]
    },
    {
      "metadata": {
        "id": "8_IaG_WjHt9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Basic RNN using LSTM blocks used to recall information over long sequences, implements \n",
        "#dropout to prevent overfitting and a sigmoidal activation func\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 250))\n",
        "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 10\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test Set score:', score)\n",
        "print('Test Set Accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vEv1cVYRdipm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It turns out that I was able to get a pretty good accuracy score on my test dataset. However in an attempt to score a little bit higher, (and show off my Pytorch skills), I will next implement a bidirectional LSTM model with attention.  I use a self attention network since the sequence is pretty long (set to 250 words per review) as well as gradient clipping to prevent the issues of exploding gradients common with long reccurent networks. I also add a regularization layer and dropout layers to prevent overfitting."
      ]
    },
    {
      "metadata": {
        "id": "PgOAmTQ5bmlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Take 2"
      ]
    },
    {
      "metadata": {
        "id": "kbyZwoOZULIm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTM_PLUS_ATTN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, attention_model, drop_prob=0.3):\n",
        "      \n",
        "        super(LSTM_PLUS_ATTN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.attention_model = attention_model\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.LSTM = nn.LSTM(embedding_dim, self.hidden_size, 1, dropout=drop_prob, batch_first=True, bidirectional = True)\n",
        "        \n",
        "        self.attention_weights1 = nn.Linear(hidden_size,hidden_size,bias=False)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.lin1 = nn.Linear(self.hidden_size*2, vocab_size/3)\n",
        "        \n",
        "        self.lin2 = nn.Linear(vocab_size/3, 1)\n",
        "        \n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        w = next(self.parameters()).data\n",
        "        \n",
        "        hidden = (w.new(2, batch_size, self.hidden_size).zero_().cuda(),\n",
        "                  w.new(2, batch_size, self.hidden_size).zero_().cuda())\n",
        "        \n",
        "        embeds = self.embedding(x)\n",
        "        \n",
        "        lstm_out, hidden = self.LSTM(embeds, hidden)        \n",
        "        \n",
        "        lin_comb, attn = self.attention_model(lstm_out)\n",
        "        \n",
        "        l1 = self.lin1(lin_comb)\n",
        "        \n",
        "        d1 = self.dropout(l1)\n",
        "        \n",
        "        l2 = self.lin2(d1)\n",
        "        \n",
        "        avg_sentence_embeddings = torch.sum(l2,1)/30\n",
        "        \n",
        "        sig_out = self.sig(avg_sentence_embeddings)\n",
        "        \n",
        "        return sig_out \n",
        "      \n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, query_dim, hidden_size):\n",
        "    \n",
        "    super(Attention, self).__init__()\n",
        "    \n",
        "    self.scale = 1. / math.sqrt(query_dim)\n",
        "    \n",
        "    self.l1 = nn.Linear(hidden_size*2, hidden_size*2)\n",
        "    \n",
        "    self.l1.bias.data.fill_(0)\n",
        "    \n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "    self.l2 = nn.Linear(hidden_size*2, 30)\n",
        "    \n",
        "    self.l2.bias.data.fill_(0)\n",
        "    \n",
        "    self.tanh = nn.Tanh()\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    l1 = self.l1(inputs)\n",
        "    \n",
        "    tanh = self.tanh(l1)\n",
        "    \n",
        "    drop = self.dropout(tanh)\n",
        "    \n",
        "    l2 = self.l2(drop)\n",
        "    \n",
        "    attn = torch.nn.functional.softmax(l2, dim=2)\n",
        "        \n",
        "    linear_combination = torch.bmm(attn.transpose(1,2), inputs).squeeze(2)\n",
        "\n",
        "    return linear_combination, attn\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZjDI08-0iZJ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "See summary of the model I used below..."
      ]
    },
    {
      "metadata": {
        "id": "MAdO5u01WnD8",
        "colab_type": "code",
        "outputId": "6482ba0d-3636-45fb-9ca1-6f9633738f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "embedding_dim = 300\n",
        "hidden_size = 256\n",
        "batch_size = 500\n",
        "epochs = 15\n",
        "\n",
        "# Data preprocessing\n",
        "train = TensorDataset(torch.from_numpy(np.array(x_train)), torch.from_numpy(np.array(y_train)))\n",
        "test = TensorDataset(torch.from_numpy(np.array(x_test)), torch.from_numpy(np.array(y_test)))\n",
        "\n",
        "# Data iterators\n",
        "train_loader = DataLoader(train, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test, shuffle=True, batch_size=batch_size)\n",
        "dataiter = iter(train_loader)\n",
        "dataiter = iter(test_loader)\n",
        "\n",
        "# Instantiate model\n",
        "attn_mod = Attention(hidden_size, hidden_size)\n",
        "model = LSTM_PLUS_ATTN(vocab_size, embedding_dim, hidden_size, attn_mod)\n",
        "print(model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM_PLUS_ATTN(\n",
            "  (attention_model): Attention(\n",
            "    (l1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (dropout): Dropout(p=0.3)\n",
            "    (l2): Linear(in_features=512, out_features=30, bias=True)\n",
            "    (tanh): Tanh()\n",
            "  )\n",
            "  (embedding): Embedding(15000, 300)\n",
            "  (LSTM): LSTM(300, 256, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (attention_weights1): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (dropout): Dropout(p=0.3)\n",
            "  (lin1): Linear(in_features=512, out_features=5000, bias=True)\n",
            "  (lin2): Linear(in_features=5000, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "im1nhRwHn5Wk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note: if the gpu is running out of vram you may need to restart the runtime and run cells 1, 2, (skip 3), 4, and 5 before running the 6th code cell."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dq462wz8-zgH",
        "outputId": "49095082-56ce-4e3a-bc55-8c6bb2a3acb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "clipping=10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "test_loss = []\n",
        "num_correct = 0\n",
        "model.cuda()\n",
        "model.train()\n",
        "print(\"training commences...\")\n",
        "for e in range(epochs):\n",
        "    counter = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        if (25000 - counter * batch_size < batch_size):\n",
        "          continue\n",
        "          \n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        model.zero_grad()\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        inputs = inputs.to(device)\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch #\" + str(e+1) + \"/\" + str(epochs) + \", \" + \"Loss: \" + str(loss.item()))\n",
        "\n",
        "val_losses = []\n",
        "model.eval()\n",
        "num_correct = 0\n",
        "for inputs, labels in test_loader:\n",
        "    #Finished the epoch\n",
        "    if(inputs.shape[0]<batch_size):\n",
        "      continue\n",
        "\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    inputs = inputs.type(torch.LongTensor)\n",
        "    inputs = inputs.to(device)\n",
        "    output = model(inputs)\n",
        "    val_loss = criterion(output.squeeze(), labels.float())\n",
        "    val_losses.append(val_loss.item())\n",
        "    pred = torch.round(output.squeeze())\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "    test_acc = num_correct/len(test_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "print(\"Test accuracy: \" + str(test_acc))      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training commences...\n",
            "Epoch #1/15, Loss: 0.555325090885\n",
            "Epoch #2/15, Loss: 0.372678458691\n",
            "Epoch #3/15, Loss: 0.269668340683\n",
            "Epoch #4/15, Loss: 0.222672760487\n",
            "Epoch #5/15, Loss: 0.204688951373\n",
            "Epoch #6/15, Loss: 0.143753200769\n",
            "Epoch #7/15, Loss: 0.107513204217\n",
            "Epoch #8/15, Loss: 0.0357235558331\n",
            "Epoch #9/15, Loss: 0.0165668465197\n",
            "Epoch #10/15, Loss: 0.00814997218549\n",
            "Epoch #11/15, Loss: 0.0106172319502\n",
            "Epoch #12/15, Loss: 0.00995420571417\n",
            "Epoch #13/15, Loss: 0.00415983935818\n",
            "Epoch #14/15, Loss: 0.097490273416\n",
            "Epoch #15/15, Loss: 0.0130673609674\n",
            "Test accuracy: 0.87356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QuAhVTuqb6Kl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As expected we acheive a better test set accuracy than the prior network. Thank you for taking the time to look through my code! "
      ]
    }
  ]
}